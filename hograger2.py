# -*- coding: utf-8 -*-
"""hograger2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15cVg4oBdqbk39GAbRfvvP5sKqBMG8fWV
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install -U langchain
# !pip install huggingface_hub
# !pip install tiktoken
# !pip install ctransformers
# !pip install accelerate
# !pip install sentence_transformers
# !pip install faiss
# !pip install faiss-cpu
# !pip install InstructorEmbedding
# !pip install -U langchain-community
# !pip install chromadb
import os
import sys
from langchain import HuggingFaceHub
from langchain.text_splitter import CharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.chains import ConversationalRetrievalChain
from langchain.llms import CTransformers
import pickle
import faiss
from langchain.vectorstores import FAISS
from langchain import PromptTemplate, HuggingFaceHub, LLMChain
import warnings
warnings.filterwarnings("ignore")
import pickle
from google.colab import drive
# drive.mount('/content/drive')
from langchain_community.document_loaders import JSONLoader
import json
from pathlib import Path
from pprint import pprint
from langchain.schema import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import Chroma

# os.environ["HUGGINGFACEHUB_API_TOKEN"] = "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"

# file_path='/content/corpus.json'
# json_data = json.loads(Path(file_path).read_text())

# embedding_model_name = "sentence-transformers/all-mpnet-base-v2"
# model_kwargs = {"device": "cuda"}
# embeddings = HuggingFaceEmbeddings(
#   model_name=embedding_model_name,
#   model_kwargs=model_kwargs
# )

# Step 1: Read JSON data
def read_json_file(file_path):
    """Reads the JSON file and returns the list of documents."""
    with open(file_path, 'r') as f:
        json_data = json.load(f)
    return json_data

# Utility function to filter out None values from metadata
def filter_metadata(metadata):
    """Filters out None values from metadata, replacing them with empty strings."""
    return {k: (v if v is not None else "") for k, v in metadata.items()}

# Step 2: Process JSON into Documents (with metadata filtering)
def process_json_to_documents(json_data):
    """Converts JSON data into langchain Document objects with filtered metadata."""
    docs = []
    for entry in json_data:
        # Extract relevant fields
        title = entry.get('title', '')
        author = entry.get('author', '')
        source = entry.get('source', '')
        published_at = entry.get('published_at', '')
        category = entry.get('category', '')
        url = entry.get('url', '')
        body = entry.get('body', '')

        # Combine all fields into a single page_content string
        content = f"Title: {title}\nAuthor: {author}\nSource: {source}\nPublished At: {published_at}\nCategory: {category}\nURL: {url}\n\n{body}"

        # Store metadata and filter None values
        metadata = filter_metadata({
            "title": title,
            "author": author,
            "source": source,
            "published_at": published_at,
            "category": category,
            "url": url
        })

        # Create a Document object with page_content and metadata
        doc = Document(page_content=content, metadata=metadata)
        docs.append(doc)

    return docs


# Step 3: Split the documents using a Text Splitter
def split_documents(docs):
    """Splits documents into smaller chunks for better embedding."""
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=100  # Adjust these values based on your needs
    )
    split_docs = text_splitter.split_documents(docs)
    return split_docs

# Step 4: Generate Embeddings and Store in Chroma
def create_embeddings_with_chroma(split_docs,embedding_model_name,model_kwargs,db):
    embeddings = HuggingFaceEmbeddings(
    model_name=embedding_model_name,
    model_kwargs=model_kwargs
    )
    vector_store = Chroma.from_documents(split_docs, embeddings,persist_directory=db)
    return vector_store

# Example Usage:

# File path to the JSON file
# file_path = '/content/corpus.json'

# json_data = read_json_file(file_path)
# docs = process_json_to_documents(json_data)
# split_docs = split_documents(docs)
# vector_store = create_embeddings_with_chroma(split_docs)

# from langchain.text_splitter import RecursiveCharacterTextSplitter
# from langchain.schema import Document


# # Initialize the text splitter
# text_splitter = RecursiveCharacterTextSplitter(chunk_size=1200, chunk_overlap=20)

# # Create documents for each JSON object, splitting based on the 'body' content
# docs = []
# for item in data:
#     # Get the body for splitting
#     body_text = item.get('body', "")

#     # Split the body text into chunks
#     split_texts = text_splitter.split_text(body_text)

#     # Create Document objects, including metadata from the JSON
#     for chunk in split_texts:
#         doc = Document(
#             page_content=chunk,
#             metadata={
#                 'title': item['title'],
#                 'author': item['author'],
#                 'source': item['source'],
#                 'published_at': item['published_at'],
#                 'category': item['category'],
#                 'url': item['url']
#             }
#         )
#         docs.append(doc)

# Now 'docs' contains all the document chunks with metadata.

# def clean_metadata(documents):
#     def sanitize_metadata(metadata):
#         if isinstance(metadata, dict):
#             return {k: (sanitize_metadata(v) if v is not None else "") for k, v in metadata.items()}
#         return metadata

#     for doc in documents:
#         if "metadata" in doc:
#             doc["metadata"] = sanitize_metadata(doc["metadata"])
#     return documents

# docs = clean_metadata(docs)

# !pip install chromadb
# from langchain.vectorstores import Chroma
# from langchain.embeddings import HuggingFaceEmbeddings  # Or use any embedding model you have
# from langchain.document_loaders import TextLoader  # Depending on how you're loading documents
# chroma_index = Chroma.from_documents(docs, embeddings)


# Use one of the Flan-T5 variants
# llm = HuggingFaceHub(
#     repo_id="google/flan-t5-large",  # Change to "flan-t5-large", "flan-t5-xl", or "flan-t5-xxl" as needed
#     model_kwargs={"temperature": 0.55, "max_length": 512}
# )

# qa_chain = ConversationalRetrievalChain.from_llm(
#     llm,
#     retriever=vector_store.as_retriever(search_kwargs={'k': 3}),
#     return_source_documents=True
# )


# Function to format the output with evidence
def format_output(response):
    answer = response.get('answer', 'No answer found')  # Handle missing 'answer'
    source_documents = response.get('source_documents', [])

    # Create a list of evidence from source documents
    evidence_list = []
    for doc in source_documents:
        evidence = {
            "title": doc.metadata.get('title', 'Unknown'),
            "author": doc.metadata.get('author', 'Unknown'),
            "url": doc.metadata.get('url', 'Unknown'),
            "source": doc.metadata.get('source', 'Unknown'),
            "category": doc.metadata.get('category', 'Unknown'),
            "published_at": doc.metadata.get('published_at', 'Unknown'),
            "fact": doc.page_content[:200]  # Limiting the fact length
        }
        evidence_list.append(evidence)

    # Return structured output
    return {
        "query": response.get('question', 'No question provided'),  # Handle missing 'question'
        "answer": answer,
        "question_type": "inference_query",
        "evidence_list": evidence_list
    }

# chat_history = []

# while True:
#     query = input('Prompt: ')
#     if query.lower() in ["exit", "quit", "q"]:
#         print('Exiting')
#         sys.exit()

#     # Run the query using qa_chain
#     result = qa_chain({'question': query, 'chat_history': chat_history})

#     # Format the output with evidence
#     structured_output = format_output(result)

#     # Display the structured output
#     print('Answer:', structured_output['answer'] + '\n')
#     print('Evidence:')
#     for evidence in structured_output['evidence_list']:
#         print(f"  - Title: {evidence['title']}")
#         print(f"    Author: {evidence['author']}")
#         print(f"    Source: {evidence['source']}")
#         print(f"    Category: {evidence['category']}")
#         print(f"    URL: {evidence['url']}")
#         print(f"    Fact: {evidence['fact'][:150]}...")  # Truncated fact for readability
#         print()

#     # Add the current query and response to chat history for context in future queries
#     chat_history.append((query, structured_output['answer']))
